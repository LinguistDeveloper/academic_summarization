{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d47d5b-b845-46cc-b227-f8aea852496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a19c05d-c409-4936-872e-dc2645ec2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('papers.SSN.jsonl', 'r', encoding='utf-8') as f:\n",
    "#    print(f)\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67ffb1f-e85f-473b-9fec-662eb727bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e37f6-cb33-4361-a8b4-6088449b67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6de98f-aa84-4b7e-945a-bf9845e1f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target domains\n",
    "target_domains = {\"Sociology\", \"History\", \"Biology\", \"Geography\", \"Geology\"}\n",
    "\n",
    "# Initialize lists\n",
    "paper_id, title, abstract, text, domain = [], [], [], [], []\n",
    "\n",
    "# Process entries\n",
    "for entry in data:\n",
    "    if 'domain' in entry and any(d in entry['domain'] for d in target_domains):\n",
    "        paper_id.append(entry['paper_id'])\n",
    "        title.append(entry['title'])\n",
    "\n",
    "        # Clean abstract\n",
    "        abstract_cl = \"\".join(\n",
    "            re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence) \n",
    "            for sublist in entry['abstract'] \n",
    "            for sentence in sublist\n",
    "        )\n",
    "        abstract.append(abstract_cl)\n",
    "\n",
    "        # Clean text\n",
    "        text_cl = \" \".join(\n",
    "            re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence) \n",
    "            for sublist in entry['text'] \n",
    "            for sentence in sublist\n",
    "        )\n",
    "        text.append(text_cl)\n",
    "\n",
    "        domain.append(entry['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839bad8-f756-4e5e-a1bf-9d0219d31ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d205a0d-f96e-4b41-984e-37f13abecbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(paper_id), len(title), len(abstract), len(text), len(domain))\n",
    "\n",
    "paper_id = paper_id[:min_length]\n",
    "title = title[:min_length]\n",
    "abstract = abstract[:min_length]\n",
    "text = text[:min_length]\n",
    "domain = domain[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73900555-cb66-4593-8e02-e2626ac83f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dataset object\n",
    "ssn_dataset = Dataset.from_dict({\n",
    "    \"paper_id\": paper_id,\n",
    "    \"title\": title,\n",
    "    \"abstract\": abstract,\n",
    "    \"text\": text,\n",
    "    \"domain\": domain\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "defba0cd-c056-4ba3-ad70-553e4e6c053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_dataset = ssn_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb6926a-6db5-4610-a3a7-fc611260af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9637385b-049c-468b-b7c3-64d1b08aba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ffe7f73-8da6-43b4-a495-458d44f51cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"abstract\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c987e5-38db-4006-af4c-7f24580dc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ssn_dataset = ssn_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55e4d9e1-cb2b-4064-b743-9297a885e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d79828a-ec90-4465-ac3a-769498d1bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57f176ce-3826-4e35-a423-9aca12af7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7ccf7-bfac-4c5b-a659-ee3c4607578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86160db5-0426-48f8-b792-65d06a4c52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./tmp_test\",  # Temporary output directory\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,  # Reduce epochs\n",
    "#    max_steps=2,  # Run only 2 steps\n",
    "    per_device_train_batch_size=16,  # Increase batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "#    per_device_train_batch_size=1,  # Smallest batch size\n",
    "#    per_device_eval_batch_size=1,  # Smallest batch size\n",
    "    eval_strategy=\"epoch\",  # No evaluation for speed\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # Keep only the 2 most recent models\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,  # Log every 200 steps\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size is doubled\n",
    "    predict_with_generate=True,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ssn_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_ssn_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb150d2-a4c1-48d9-8d11-f1e769560e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f17e5-512f-4a35-b913-bf3928f7006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a4af3f-78e9-46f8-8e06-05c43f30857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: Ten settlements from the culture have been found.[1] The first six sites discovered were: the type site at Baodun in Xinjin County, the site at Mangcheng in Dujiangyan City, the site at Yufu in Wenjiang County, the site at Zizhu in Chongzhou, the site at Shuanghe in Chongzhou, and the site at Gucheng in Pi County. Yufucun is the second largest site associated with the Baodun culture. All of the settlements straddle the Min River. The settlement walls were covered with pebbles, a feature unique to the Baodun culture. The pottery from the culture share some similarities with Sanxingdui. The inhabitants lived in wattle and daub houses.[2] The earliest evidence for rice and foxtail millet agriculture in southwest China was discovered at the type site at Baodun.[1]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5de98-518b-42f2-8ae1-89893eebc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"rcook/tmp_test\")\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0fd5c-e52d-493c-959c-18449423a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb55a1-ac2c-41df-a7e0-b05d894bec39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
